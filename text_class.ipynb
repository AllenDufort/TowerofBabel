{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (0.12.1)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from seaborn) (1.4.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from seaborn) (3.5.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from seaborn) (1.19.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.37.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from pandas>=0.25->seaborn) (2022.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from nltk) (4.64.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (1.8.2.2)\n",
      "Requirement already satisfied: matplotlib in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from wordcloud) (3.5.3)\n",
      "Requirement already satisfied: pillow in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from wordcloud) (1.19.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from matplotlib->wordcloud) (4.37.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/allendufort/opt/miniconda3/envs/DL3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import make_scorer, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"BBC News Train.csv\")\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the given news categories into categorical values.\n",
    "target_category = dataset['Category'].unique()\n",
    "print(target_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Category'].factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate Category names with numerical index and save it in new column CategoryId\n",
    "dataset['CategoryId'] = dataset['Category'].factorize()[0]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new pandas dataframe \"category\", which only has unique Categories, also sorting this list in order of CategoryId values\n",
    "category = dataset[['Category', 'CategoryId']].drop_duplicates().sort_values('CategoryId')\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.Category.value_counts().plot(kind = \"bar\", color = [\"purple\", \"orange\", \"red\", \"green\", \"blue\"])\n",
    "plt.xlabel(\"Category of data\")\n",
    "plt.title(\"Articles Assigned to Categories\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,5))\n",
    "colors = [\"purple\", \"blue\", \"red\", \"green\", \"orange\"]\n",
    "business = dataset[dataset['CategoryId'] == 0 ]\n",
    "tech = dataset[dataset['CategoryId'] == 1 ]\n",
    "politics = dataset[dataset['CategoryId'] == 2]\n",
    "sport = dataset[dataset['CategoryId'] == 3]\n",
    "entertainment = dataset[dataset['CategoryId'] == 4]\n",
    "count = [business['CategoryId'].count(), tech['CategoryId'].count(), politics['CategoryId'].count(), sport['CategoryId'].count(), entertainment['CategoryId'].count()]\n",
    "pie = plt.pie(count, labels = ['business', 'tech', 'politics', 'sport', 'entertainment'],\n",
    "              autopct = \"%1.1f%%\",\n",
    "              shadow = True,\n",
    "              colors = colors,\n",
    "              startangle = 90,\n",
    "              explode = (0.05, 0.05, 0.05, 0.05,0.05))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "business = dataset[dataset['CategoryId'] == 0]\n",
    "business = business['Text']\n",
    "\n",
    "tech = dataset[dataset['CategoryId'] == 1]\n",
    "tech = tech['Text']\n",
    "\n",
    "politics = dataset[dataset['CategoryId'] == 2]\n",
    "politics = politics['Text']\n",
    "\n",
    "sport = dataset[dataset['CategoryId'] == 3]\n",
    "sport = sport['Text']\n",
    "\n",
    "entertainment = dataset[dataset['CategoryId'] == 4]\n",
    "entertainment = entertainment['Text']\n",
    "\n",
    "def wordcloud_draw(dataset, color = 'white'):\n",
    "\n",
    "    words = ' '.join(dataset)\n",
    "\n",
    "    cleaned_word = ' '.join([word for word in words.split() if (word != 'news' and word != 'text')])\n",
    "\n",
    "    wordcloud = WordCloud(stopwords = stop, \n",
    "                          background_color = color,\n",
    "                          width = 400, \n",
    "                          height = 400).generate(cleaned_word)\n",
    "\n",
    "    plt.figure(1, figsize = (10,7))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"business related words:\")\n",
    "wordcloud_draw(business, 'white')\n",
    "\n",
    "print(\"tech related words:\")\n",
    "wordcloud_draw(tech, 'white')\n",
    "\n",
    "print(\"politics related words:\")\n",
    "wordcloud_draw(politics, 'white')\n",
    "\n",
    "print(\"sport related words:\")\n",
    "wordcloud_draw(sport, 'white')\n",
    "\n",
    "print(\"entertainment related words:\")\n",
    "wordcloud_draw(entertainment, 'white')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all tags in dataset\n",
    "def remove_tags(text):\n",
    "  remove = re.compile(r'')\n",
    "  return re.sub(remove, '', text)\n",
    "dataset['Text'] = dataset['Text'].apply(remove_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all special characters in dataset\n",
    "def special_char(text):\n",
    "  reviews = ''\n",
    "  for x in text:\n",
    "    if x.isalnum():\n",
    "      reviews = reviews + x\n",
    "    else:\n",
    "      reviews = reviews + ' '\n",
    "  return reviews\n",
    "dataset['Text'] = dataset['Text'].apply(special_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower(text):\n",
    "   return text.lower()\n",
    "dataset['Text'] = dataset['Text'].apply(convert_lower)\n",
    "dataset['Text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  words = word_tokenize(text)\n",
    "  return [x for x in words if x not in stop_words]\n",
    "dataset['Text'] = dataset['Text'].apply(remove_stopwords)\n",
    "dataset['Text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(text):\n",
    "  wordnet = WordNetLemmatizer()\n",
    "  return \" \".join([wordnet.lemmatize(word) for word in text])\n",
    "dataset['Text'] = dataset['Text'].apply(lemmatize_word)\n",
    "dataset['Text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "x = np.array(dataset.iloc[:,0].values)\n",
    "y = np.array(dataset.CategoryId.values)\n",
    "cv = CountVectorizer(max_features = 5000)\n",
    "x = cv.fit_transform(dataset.Text).toarray()\n",
    "print(\"X.shape = \",x.shape)\n",
    "print(\"y.shape = \",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of model and accuracy dicts\n",
    "perform_list = [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_name, est_c, est_pnlty):\n",
    "\n",
    "    mdl=''\n",
    "\n",
    "    if model_name == 'Logistic Regression':\n",
    "        mdl = LogisticRegression()\n",
    "    elif model_name == 'Random Forest':\n",
    "        mdl = RandomForestClassifier(n_estimators=100 ,criterion='entropy' , random_state=0)\n",
    "    elif model_name == 'Multinomial Naive Bayes':\n",
    "        mdl = MultinomialNB(alpha=1.0,fit_prior=True)\n",
    "    elif model_name == 'Support Vector Classifier':\n",
    "        mdl = SVC()\n",
    "    elif model_name == 'Decision Tree Classifier':\n",
    "        mdl = DecisionTreeClassifier()\n",
    "    elif model_name == 'K Nearest Neighbor k=10':\n",
    "        mdl = KNeighborsClassifier(n_neighbors=10 , metric= 'minkowski' , p = 4)\n",
    "    elif model_name == 'K Nearest Neighbor k=7':\n",
    "        mdl = KNeighborsClassifier(n_neighbors=7 , metric= 'minkowski' , p = 4)\n",
    "    elif model_name == 'Gaussian Naive Bayes':\n",
    "        mdl = GaussianNB()\n",
    "    elif model_name == 'Multilayer Perceptron Classifier':\n",
    "        mdl = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(6,), random_state=1)\n",
    "        \n",
    "\n",
    "    oneVsRest = OneVsRestClassifier(mdl)\n",
    "\n",
    "    oneVsRest.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = oneVsRest.predict(x_test)\n",
    "\n",
    "    # Performance metrics\n",
    "    accuracy = round(accuracy_score(y_test, y_pred) * 100, 2)\n",
    "\n",
    "    # Get precision, recall, f1 scores\n",
    "    precision, recall, f1score, support = score(y_test, y_pred, average='micro')\n",
    "\n",
    "    print(f'Test Accuracy Score of Basic {model_name}: % {accuracy}')\n",
    "    print(f'Precision : {precision}')\n",
    "    print(f'Recall : {recall}')\n",
    "    print(f'F1-score : {f1score}')\n",
    "\n",
    "    # Add performance parameters to list\n",
    "    perform_list.append(dict([\n",
    "        ('Model', model_name),\n",
    "        ('Test Accuracy', round(accuracy, 2)),\n",
    "        ('Precision', round(precision, 2)),\n",
    "        ('Recall', round(recall, 2)),\n",
    "        ('F1', round(f1score, 2))\n",
    "        ])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model('Logistic Regression', est_c=None, est_pnlty=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model('Random Forest', est_c=None, est_pnlty=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model('Multinomial Naive Bayes', est_c=None, est_pnlty=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model('Support Vector Classifier', est_c=None, est_pnlty=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model('Decision Tree Classifier', est_c=None, est_pnlty=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model('Gaussian Naive Bayes', est_c=None, est_pnlty=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model('K Nearest Neighbor k=10', est_c=None, est_pnlty=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model('K Nearest Neighbor k=7', est_c=None, est_pnlty=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This a neural network\n",
    "run_model('Multilayer Perceptron Classifier', est_c=None, est_pnlty=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_performance = pd.DataFrame(data=perform_list)\n",
    "model_performance = model_performance[['Model', 'Test Accuracy', 'Precision', 'Recall', 'F1']]\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = RandomForestClassifier(n_estimators=100 ,criterion='entropy' , random_state=0).fit(x_train, y_train)\n",
    "# classifier\n",
    "# y_pred = classifier.predict(x_test)\n",
    "\n",
    "mdl0 = LogisticRegression()\n",
    "\n",
    "mdl1 = RandomForestClassifier(n_estimators=100 ,criterion='entropy' , random_state=0)\n",
    "\n",
    "mdl2 = MultinomialNB(alpha=1.0,fit_prior=True)\n",
    "\n",
    "mdl3 = SVC()\n",
    "\n",
    "mdl4 = DecisionTreeClassifier()\n",
    "\n",
    "mdl5 = GaussianNB()\n",
    "\n",
    "mdl6 = KNeighborsClassifier(n_neighbors=10 , metric= 'minkowski' , p = 4)\n",
    "\n",
    "mdl7 = KNeighborsClassifier(n_neighbors=7 , metric= 'minkowski' , p = 4)\n",
    "\n",
    "mdl8 = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(6,), random_state=1)\n",
    "\n",
    "classifier = OneVsRestClassifier(mdl5)\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = cv.transform(['I took a film class for actors']).toarray()\n",
    "yy = classifier.predict(y_pred1)\n",
    "result = \"\"\n",
    "if yy == [0]:\n",
    "  result = \"Business News\"\n",
    "elif yy == [1]:\n",
    "  result = \"Tech News\"\n",
    "elif yy == [2]:\n",
    "  result = \"Politics News\"\n",
    "elif yy == [3]:\n",
    "  result = \"Sports News\"\n",
    "elif yy == [4]:\n",
    "  result = \"Entertainment News\"\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I set the default classifier to the most accurate classifier\n",
    "\n",
    "def phrase_category(phrase, model = RandomForestClassifier(n_estimators=100 ,criterion='entropy' , random_state=0)):\n",
    "    classifier = OneVsRestClassifier(model)\n",
    "    classifier.fit(x_train, y_train)\n",
    "    \n",
    "    y_pred = cv.transform([phrase]).toarray()\n",
    "    yy = classifier.predict(y_pred)\n",
    "    \n",
    "    result = \"\"\n",
    "    if yy == [0]:\n",
    "      result = \"Business News\"\n",
    "    elif yy == [1]:\n",
    "      result = \"Tech News\"\n",
    "    elif yy == [2]:\n",
    "      result = \"Politics News\"\n",
    "    elif yy == [3]:\n",
    "      result = \"Sports News\"\n",
    "    elif yy == [4]:\n",
    "      result = \"Entertainment News\"\n",
    "    \n",
    "    print(f\"\\nThe model: {model}\")\n",
    "    print(f\"The phrase: '{phrase[:30]}...'\")\n",
    "    print(f\"The category: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [mdl0, mdl1, mdl2, mdl3, mdl4, mdl5, mdl6, mdl7, mdl8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual category: sports\n",
    "string = \"I want to play kickball\"\n",
    "\n",
    "phrase_category(string)\n",
    "\n",
    "for mdl_i in model_list:\n",
    "    phrase_category(string, mdl_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual category: for fun\n",
    "sentence = \"high schoolers are always depressed\"\n",
    "for mdl_i in model_list:\n",
    "    phrase_category(sentence, mdl_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual category: tech news\n",
    "sentence = \"Subject: Re: Electronic Tesla Coils First of all, realize that Tesla invented AC power generators, motors, transformers, conductors, etc. Technically, *ALL* transformers are Tesla coils.  In general though when someone refers to a Tesla coil, they mean an 'air core resonant transformer'. The TV flyback version Tesla coil (see the _Encyclopedia_of_Electronic_Circuits_ V3, 106-1 for diagram) has NOT an air core. It is of a class of circuit called 'Oscillating Shuttle Circuit' (OSC). Generally OSC's are highly efficient, but this version uses transistors and resistors, which are very lossy devices. Typically Tesla used active reactances instead of passive resistors, so that he could achieve efficiencies of 99.5%, and better. The usual application of an air-core resonant transformer, or of an OSC, is to produce strong EMI for wireless broadcasts. How well do you think your computer screen would work if we removed the HF HV Tesla (flyback) coil from it? If we were to remove from our homes and industries all Tesla coils, our lights would go dark, our cars would sputter and die, our radios would go silent, our industries would grind to a halt, and we would have to go back to using coal for heat, gas for lamps, horses for transportation, steam for power, and telegraph for communication. Is that real world enough for you??????? GET THE MESSAGE! WE WOULD NOT HAVE 1/100 THE CONVIENIENCES WE HAVE TODAY IF NOT FOR TESLA. GIVE CREDIT WHERE CREDIT IS DUE! If it had been up to Edison, we'd still be in the 19th century. (flame me at your own peril. I'm very good at putting edison down).  'Tesla was 100 years ahead of his time. Perhaps now his time comes'.\"\n",
    "for mdl_i in model_list:\n",
    "    phrase_category(sentence, mdl_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual category: political paper \n",
    "# directory :useless files/20-newsgroups/talk.politics.misc/176845\n",
    "sentence = \"Furthermore, what are the specific charges against the four LAPD officers? Which civil rights or laws are they accused of violating? I believe it is a general charge, that is no specific right is mentioned. I don't think that this is accurate. I believe, and could be wrong, that there IS a specific right allegedly to have been violated, like the 14th or due process or whatever. What about double jeopardy? Has there been any concern that a verdict against Koon, et al. might be overturned upon appeal because they're being tried again for the same actions? (I thought I heard something on the news about this.) The SS has previously ruled that since the seperate governments were in essence seperate sovereigns, then double jeopardy does not apply. (If this is true, then could defendents also be tried under city and county governments?) This mornings paper said that the ACLU has decided to reinstate itsopposition to this kind of thing. They had earlier suspended their opposition while they examined the King case. There might be hope for the ACLU after all. Double jeopardy does not apply, but not for the reasons you quote. Double jeopardy states that a person may not be tried twice on the same charge. However, the police are not on trial for the crime of excessive force or assault. They are NOW on trial for the DIFFERENT crime of violating Mr. King's civil rights. AS for the city and county or state trying you more than once, it most likely will not happen. This is because cities and states have separate laws governing behaviour. For example, in some states, it is an offence to carry marijuana, but not a city offence. Also, I think murder is against federal, but not some state laws.\"\n",
    "for mdl_i in model_list:\n",
    "    phrase_category(sentence, mdl_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most accurate at classifications in outside tests are:\n",
    "    # [Index | Classification]\n",
    "    # [1     | RandomForestClassifier]\n",
    "    # [2     | MultinomialNB]\n",
    "    # [7     | MLPClassifier]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL3 (3.9)",
   "language": "python",
   "name": "dl3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e774977668b7c0ae8309835a5187aa7fbf7669e7d0bb59755bc63e573643edcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
